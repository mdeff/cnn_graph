{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Traffic\n",
    "\n",
    "This notebook aims at modeling user traffic on [Wikipedia](https://wikipedia.org) using a recurrent graph convolutional neural network.\n",
    "\n",
    "Goal: anomaly detection. Can be used to detect events in the real world. Other applications:\n",
    "* intrusion detection on telecomunnication networks,\n",
    "* anomaly detection on energy networks,\n",
    "* accident detection on transporation networks.\n",
    "\n",
    "Events: Super Bowl, Academy Awards, Grammy, Miss Universe, Golden Globe. Mostly December-February.\n",
    "Missed: Charlie Hebdo, Ebola\n",
    "\n",
    "Network is very large: 5M nodes, 300M edges. Downsampling ideas:\n",
    "* Choose a category, e.g. science.\n",
    "* Take most active ones.\n",
    "* Concatenate in modules / communities / super-nodes.\n",
    "\n",
    "Raw data\n",
    "* [Wikimedia SQL dumps](https://dumps.wikimedia.org/enwiki/), to construct the hyperlink graph.\n",
    "    * Network size: 5M nodes, 300M edges.\n",
    "* [Pagecounts](https://dumps.wikimedia.org/other/pagecounts-all-sites/) as activations on the graph.\n",
    "    * Data from 2014-09-23 0h to 2015-06-05 22h.\n",
    "    * 6142 hours in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import IPython.display as ipd\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import graph_tool.all as gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "#WIKI_RAW = os.environ.get('WIKI_RAW')  # Downloaded from dumps.wikimedia.org.\n",
    "#WIKI_CLEAN = os.environ.get('WIKI_CLEAN')  # Processed by Kirell Benzi.\n",
    "\n",
    "DATA_DIR = os.path.join('..', 'data', 'wikipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "plt.rcParams['figure.figsize'] = (17, 5)\n",
    "plt.rcParams['agg.path.chunksize'] = 10000  # OverflowError when plotting large series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Hyperlink graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = gt.load_graph(os.path.join(DATA_DIR, 'enwiki-20150403-graph.gt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.is_directed()\n",
    "#graph.set_directed(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graph(graph):\n",
    "    print('{} vertices, {} edges'.format(\n",
    "        graph.num_vertices(), graph.num_edges()))\n",
    "\n",
    "print_graph(graph)\n",
    "graph.list_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 42\n",
    "page_title = graph.vertex_properties['page_title'][idx]\n",
    "page_id = graph.vertex_properties['page_id'][idx]\n",
    "print('{}: {}'.format(page_id, page_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_degree_distribution(graph):\n",
    "    hist = gt.vertex_hist(graph, 'total')\n",
    "    plt.loglog(hist[1][:-1], hist[0])\n",
    "    plt.xlabel('#edges')\n",
    "    plt.ylabel('#nodes')\n",
    "    #plt.savefig('degree_distribution.pdf')\n",
    "plot_degree_distribution(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Pages\n",
    "\n",
    "A lot of pages in `pagecounts` are redirections to actual pages. We need to merge the hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(DATA_DIR, 'enwiki-20150403-page-redirect.csv.gz')\n",
    "redirect = pd.read_csv(filepath, compression='gzip', sep='|', encoding='utf-8', quoting=3, index_col=1)\n",
    "\n",
    "redirect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert len(redirect) == len(redirect['page_id'].unique())\n",
    "print('{:.2e} unique pages, {:.2e} pages including redirections'.format(\n",
    "        len(redirect['fix_page_id'].unique()),\n",
    "        len(redirect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect.loc[page_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2title(page_id):\n",
    "    page_title = redirect.at[page_id, 'fix_page_title']\n",
    "    #page_title = graph.vp['page_title'][id]\n",
    "    print('{}: https://en.wikipedia.org/?curid={}'.format(page_title, page_id))\n",
    "    return page_title\n",
    "id2title(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_in_title(string):\n",
    "\n",
    "    def find(page_title, string):\n",
    "        try:\n",
    "            return string.lower() in page_title.lower()\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    #b = redirect['fix_page_title'].apply(find, string=string)\n",
    "    b = redirect['page_title'].apply(find, string=string)\n",
    "    #return redirect[b]\n",
    "    return redirect[b & (redirect['is_redirect'] == 0)]\n",
    "\n",
    "find_in_title('ebola')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Page views / counts\n",
    "\n",
    "Graph has 4M nodes but lot of pages are not seen much. `signal_500.h5` lists only 118k pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kirell's signal which includes views when greater than 500.\n",
    "filepath = os.path.join(DATA_DIR, 'signal_500.h5')\n",
    "signal = pd.read_hdf(filepath, 'data')\n",
    "signal['count_views'].plot(kind='hist', logy=True)\n",
    "print(len(signal), len(signal['page_id'].unique()), len(signal['layer'].unique()), signal['count_views'].max())\n",
    "signal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pagecounts(date):\n",
    "    filename = 'pagecounts-{:4d}{:02d}{:02d}-{:02d}0000.csv.gz'.format(date.year, date.month, date.day, date.hour)\n",
    "    filepath = os.path.join('..', 'data', 'wikipedia', 'pagecounts_clean', filename)\n",
    "    return pd.read_csv(filepath, compression='gzip', index_col=0, squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = datetime.datetime(2014, 9, 23, 2)\n",
    "END = datetime.datetime(2014, 9, 24, 3)\n",
    "END = datetime.datetime(2015, 6, 5, 20)\n",
    "dates = pd.date_range(START, END, freq='H')\n",
    "\n",
    "activations_tot = pd.Series(\n",
    "    data=0,\n",
    "    index=graph.vp['page_id'].get_array(),\n",
    "    dtype=np.int64\n",
    ")\n",
    "\n",
    "for date in tqdm_notebook(dates):\n",
    "    pagecounts = get_pagecounts(date)\n",
    "    activations_tot += pagecounts.reindex(activations_tot.index).fillna(0).astype(np.int32)\n",
    "\n",
    "print(activations_tot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The largest is the main page.\n",
    "plt.semilogy(np.sort(activations_tot.values)[::-1])\n",
    "\n",
    "main_page = activations_tot.argmax()\n",
    "print('{} ({}): {:.2e} views in total'.format(id2title(main_page), main_page, activations_tot[main_page]))\n",
    "\n",
    "print('{:.2e} views in total'.format(activations_tot.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power law.\n",
    "activations_tot.drop(main_page).plot(kind='hist', logy=True, bins=100);\n",
    "plt.figure()\n",
    "activations_tot.drop(main_page)[activations_tot < 1e7].plot(kind='hist', logy=True, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_AVG_VIEWS = 100\n",
    "\n",
    "keep = activations_tot.index[activations_tot >= MIN_AVG_VIEWS * len(dates)]\n",
    "print('{} pages have more than {} views in total ({:.0f} per hour on average)'.format(\n",
    "    len(keep), MIN_AVG_VIEWS * len(dates), MIN_AVG_VIEWS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = pd.DataFrame(\n",
    "    data=0,\n",
    "    index=keep,\n",
    "    columns=dates,\n",
    "    dtype=np.int32\n",
    ")\n",
    "\n",
    "for date in tqdm_notebook(dates):\n",
    "    pagecounts = get_pagecounts(date)\n",
    "    activations[date] = pagecounts.reindex(activations.index).fillna(0).astype(np.int32)\n",
    "\n",
    "activations.sort_index(inplace=True)\n",
    "\n",
    "filepath = os.path.join('..', 'data', 'wikipedia', 'activations_{}.h5'.format(MIN_AVG_VIEWS))\n",
    "activations.to_hdf(filepath, 'activations')\n",
    "\n",
    "ipd.display(activations.head())\n",
    "ipd.display(activations.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Predictable fluctuations with unpredictable spikes. Those are outliers.\n",
    "* Anomalies should be outliers persisting for many hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP = [\n",
    "   15580374,  # Main page draws ~10% traffic.\n",
    "   42727860,  # Undefined has the largest peaks of traffic while being inactive after 2014-10.\n",
    "#   8063851,   # Feynman point has a very large traffic peak which is probably an error.\n",
    "#   2697304,   # Gold_as_an_investment has many traffic peaks.\n",
    "]\n",
    "\n",
    "def load_activations(filepath, drop=DROP):\n",
    "    activations = pd.read_hdf(filepath, 'activations')\n",
    "\n",
    "    if drop:\n",
    "        activations.drop(drop, inplace=True)\n",
    "    \n",
    "    print('activations: {} page ids x {} hours = {}'.format(*activations.shape, activations.size))\n",
    "    return activations\n",
    "\n",
    "activations = load_activations(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max of {0} views at page id {2} and time {1}'.format(\n",
    "    activations.unstack().max(), *activations.unstack().argmax())) \n",
    "plt.plot(activations.values.reshape(-1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(activations.values.reshape(-1), bins=100, log=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activation(page_id):\n",
    "    page_title = id2title(page_id)\n",
    "    ax = activations.loc[page_id].plot(label='{} ({})'.format(page_title, page_id), logy=True)\n",
    "    ax.set_ylabel('#views per hour');\n",
    "    ax.legend()\n",
    "    #plt.savefig('{}_{}.png'.format(page_id, page_title.lower()), dpi=300)\n",
    "    #plt.savefig('{}_{}.pdf'.format(page_id, page_title.lower()))\n",
    "\n",
    "# Events.\n",
    "plot_activation(2251390)   # Charlie Hebdo\n",
    "plot_activation(44969225)  # Charlie Hebdo shooting\n",
    "plt.figure()\n",
    "plot_activation(27718)     # Super Bowl\n",
    "plt.figure()\n",
    "#plot_activation(40817806)  # Ebola\n",
    "plot_activation(44635)     # Grammy\n",
    "plot_activation(150340)    # Miss Universe\n",
    "#plot_activation(324)       # Academy Awards\n",
    "\n",
    "# Neighbors of Charlie Hebdo.\n",
    "#plot_activation(44969610)  # Charb\n",
    "#plot_activation(206682)    # Caricature\n",
    "#plot_activation(15012)     # Islamism\n",
    "#plot_activation(7826589)   # Jihadism\n",
    "#plot_activation(50100)     # Journalist\n",
    "\n",
    "# Remarkable things.\n",
    "#plot_activation(25)\n",
    "#plot_activation(15580374)  # Main Page --> largest traffic (~10%)\n",
    "#plot_activation(42727860)  # Undefined --> hits only before mid-oct 2014\n",
    "#plot_activation(670)       # Alphabet --> strange drop\n",
    "#plot_activation(8063851)   # Shall distinguish outliers (counting errors?) from real events\n",
    "#plot_activation(2697304)   # Lots of peaks --> correlated with fluctuations on market?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Matching graph & activations\n",
    "\n",
    "Further analysis\n",
    "* Ratio of in / out neighbors.\n",
    "* Proportion of bidirectional hyperlinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = load_activations(os.path.join(DAT_DIR, 'activations_100.h5'))\n",
    "graph = gt.load_graph(os.path.join(DATA_DIR, 'enwiki-20150403-graph.gt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diameter(graph):\n",
    "    d = gt.pseudo_diameter(graph)[0]\n",
    "    print('Pseudo-diameter: {}'.format(int(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_graph(graph)\n",
    "compute_diameter(graph)\n",
    "\n",
    "mask = np.in1d(graph.vp['page_id'].get_array(), activations.index)\n",
    "graph = gt.GraphView(graph, vfilt=mask)\n",
    "print_graph(graph)\n",
    "\n",
    "l = gt.label_largest_component(graph)\n",
    "graph = gt.GraphView(graph, vfilt=l)\n",
    "print_graph(graph)\n",
    "compute_diameter(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = gt.Graph(graph, prune=True)\n",
    "\n",
    "def sort_vertices(graph, vp):\n",
    "    sort = np.argsort(vp.get_array())\n",
    "    sort = np.argsort(sort)\n",
    "    sort = graph.new_vertex_property('int64_t', sort)\n",
    "    return gt.Graph(graph, vorder=sort)\n",
    "\n",
    "graph = sort_vertices(graph, graph.vp['page_id'])\n",
    "# directed=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = activations.loc[graph.vp['page_id'].get_array()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_equal(graph.vp['page_id'].get_array(), activations.index)\n",
    "\n",
    "graph.save(os.path.join('..', 'data', 'wikipedia', 'graph.gt'))\n",
    "graph.save(os.path.join('..', 'data', 'wikipedia', 'graph.graphml'))\n",
    "activations.to_hdf(os.path.join('..', 'data', 'wikipedia', 'activations.h5'), 'activations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gt.sfdp_layout()\n",
    "#gt.graph_draw(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = gt.load_graph(os.path.join(DATA_DIR, 'graph.gt'))\n",
    "activations = load_activations(os.path.join(DATA_DIR, 'activations.h5'), drop=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_degree_distribution(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adjacency(graph, ax=None):\n",
    "    A = gt.adjacency(graph)\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.spy(A[:10000,:10000], markersize=0.2)\n",
    "    ax.set_title('{} nodes, {} edges ({:.2%})'.format(\n",
    "        A.shape[0], A.nnz, A.nnz / np.multiply(*A.shape)))\n",
    "\n",
    "plot_adjacency(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_adjacency_plot(graph, ax=None, **kwargs):\n",
    "    state = gt.minimize_blockmodel_dl(graph, **kwargs)\n",
    "    graph = sort_vertices(graph, state.get_blocks())\n",
    "    plot_adjacency(graph, ax)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3)\n",
    "for ax, n_blocks in zip(axes, [10, 20, 30]):\n",
    "    order_adjacency_plot(graph, ax=ax, B_max=n_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(activations.values.reshape(-1), bins=100, log=True);\n",
    "plt.figure()\n",
    "plt.hist(activations.sum(axis=1).values.reshape(-1), bins=100, log=True);"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}