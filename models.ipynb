{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import sklearn\n",
    "import graph\n",
    "import os, time, collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NFEATURES = 28**2\n",
    "#NCLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common methods for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class base_model(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.regularizers = []\n",
    "    \n",
    "    # High-level interface which runs the constructed computational graph.\n",
    "    \n",
    "    def predict(self, data, labels=None, sess=None):\n",
    "        # Restore parameters if no session given.\n",
    "        if sess is None:\n",
    "            sess = tf.Session(graph=self.graph)\n",
    "            filename = tf.train.latest_checkpoint(os.path.join('checkpoints', self.dir_name))\n",
    "            self.op_saver.restore(sess, filename)\n",
    "\n",
    "        loss = 0\n",
    "        size = data.shape[0]\n",
    "        predictions = np.empty(size)\n",
    "        for begin in range(0, size, self.batch_size):\n",
    "            end = begin + self.batch_size\n",
    "            end = min([end, size])\n",
    "            \n",
    "            batch_data = np.zeros((self.batch_size, data.shape[1]))\n",
    "            tmp_data = data[begin:end,:]\n",
    "            if type(tmp_data) is not np.ndarray:\n",
    "                tmp_data = tmp_data.toarray()  # convert sparse matrices\n",
    "            batch_data[:end-begin] = tmp_data\n",
    "            feed_dict = {self.ph_data: batch_data, self.ph_dropout: 1}\n",
    "            \n",
    "            # Compute loss if labels are given.\n",
    "            if labels is not None:\n",
    "                batch_labels = np.zeros(self.batch_size)\n",
    "                batch_labels[:end-begin] = labels[begin:end]\n",
    "                feed_dict[self.ph_labels] = batch_labels\n",
    "                batch_pred, batch_loss = sess.run([self.op_prediction, self.op_loss], feed_dict)\n",
    "                loss += batch_loss\n",
    "            else:\n",
    "                batch_pred = sess.run(self.op_prediction, feed_dict)\n",
    "            \n",
    "            predictions[begin:end] = batch_pred[:end-begin]\n",
    "            \n",
    "        if labels is not None:\n",
    "            return predictions, loss * self.batch_size / size\n",
    "        else:\n",
    "            return predictions\n",
    "        \n",
    "    def evaluate(self, data, labels, sess=None):\n",
    "        \"\"\"\n",
    "        Runs one evaluation against the full epoch of data.\n",
    "        Return the precision and the number of correct predictions.\n",
    "        Batch evaluation saves memory and enables this to run on smaller GPUs.\n",
    "\n",
    "        sess: the session in which the model has been trained.\n",
    "        op: the Tensor that returns the number of correct predictions.\n",
    "        data: size N x M\n",
    "            N: number of signals (samples)\n",
    "            M: number of vertices (features)\n",
    "        labels: size N\n",
    "            N: number of signals (samples)\n",
    "        \"\"\"\n",
    "        t_process, t_wall = time.process_time(), time.time()\n",
    "        predictions, loss = self.predict(data, labels, sess)\n",
    "        #print(predictions)\n",
    "        ncorrects = sum(predictions == labels)\n",
    "        accuracy = 100 * sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        f1 = 100 * sklearn.metrics.f1_score(labels, predictions, average='weighted')\n",
    "        string = 'accuracy: {:.2f} ({:d} / {:d}), f1 (weighted): {:.2f}, loss: {:.2e}'.format(\n",
    "                accuracy, ncorrects, len(labels), f1, loss)\n",
    "        if sess is None:\n",
    "            string += '\\ntime: {:.0f}s (wall {:.0f}s)'.format(time.process_time()-t_process, time.time()-t_wall)\n",
    "        return string, accuracy, f1, loss\n",
    "\n",
    "    def fit(self, train_data, train_labels, val_data=None, val_labels=None):\n",
    "        t_process, t_wall = time.process_time(), time.time()\n",
    "        sess = tf.Session(graph=self.graph)\n",
    "        writer = tf.train.SummaryWriter(os.path.join('summaries', self.dir_name), self.graph)\n",
    "        path = os.path.join('checkpoints', self.dir_name)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        path = os.path.join(path, 'model')\n",
    "        sess.run(self.op_init)\n",
    "\n",
    "        # Training.\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        indices = collections.deque()\n",
    "        num_steps = int(self.num_epochs * train_data.shape[0] / self.batch_size)\n",
    "        for step in range(1, num_steps+1):\n",
    "\n",
    "            # Be sure to have used all the samples before using one a second time.\n",
    "            if len(indices) < self.batch_size:\n",
    "                indices.extend(np.random.permutation(train_data.shape[0]))\n",
    "            idx = [indices.popleft() for i in range(self.batch_size)]\n",
    "\n",
    "            batch_data, batch_labels = train_data[idx,:], train_labels[idx]\n",
    "            if type(batch_data) is not np.ndarray:\n",
    "                batch_data = batch_data.toarray()  # convert sparse matrices\n",
    "            feed_dict = {self.ph_data: batch_data, self.ph_labels: batch_labels, self.ph_dropout: self.dropout}\n",
    "            learning_rate, loss_average = sess.run([self.op_train, self.op_loss_average], feed_dict)\n",
    "\n",
    "            # Periodical evaluation of the model.\n",
    "            if val_data is not None and (step % self.eval_frequency == 0 or step == num_steps):\n",
    "                epoch = step * self.batch_size / train_data.shape[0]\n",
    "                print('step {} / {} (epoch {:.2f} / {}):'.format(step, num_steps, epoch, self.num_epochs))\n",
    "                print('  learning_rate = {:.2e}, loss_average = {:.2e}'.format(learning_rate, loss_average))\n",
    "                string, accuracy, f1, loss = self.evaluate(val_data, val_labels, sess)\n",
    "                accuracies.append(accuracy)\n",
    "                losses.append(loss)\n",
    "                print('  validation {}'.format(string))\n",
    "                print('  time: {:.0f}s (wall {:.0f}s)'.format(time.process_time()-t_process, time.time()-t_wall))\n",
    "\n",
    "                # Summaries for TensorBoard.\n",
    "                summary = tf.Summary()\n",
    "                summary.ParseFromString(sess.run(self.op_summary, feed_dict))\n",
    "                summary.value.add(tag='validation/accuracy', simple_value=accuracy)\n",
    "                summary.value.add(tag='validation/f1', simple_value=f1)\n",
    "                summary.value.add(tag='validation/loss', simple_value=loss)\n",
    "                writer.add_summary(summary, step)\n",
    "                \n",
    "                # Save model parameters (for evaluation).\n",
    "                self.op_saver.save(sess, path, global_step=step)\n",
    "\n",
    "        print('validation accuracy: peak = {:.2f}, mean = {:.2f}'.format(max(accuracies), np.mean(accuracies[-10:])))\n",
    "        writer.close()\n",
    "        sess.close()\n",
    "        \n",
    "        t_step = (time.time() - t_wall) / num_steps\n",
    "        return accuracies, losses, t_step\n",
    "\n",
    "    # Methods to construct the computational graph.\n",
    "    \n",
    "    def build_graph(self, M_0):\n",
    "        \"\"\"Build the computational graph of the model.\"\"\"\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # Inputs.\n",
    "            with tf.name_scope('inputs'):\n",
    "                self.ph_data = tf.placeholder(tf.float32, (self.batch_size, M_0), 'data')\n",
    "                self.ph_labels = tf.placeholder(tf.int32, (self.batch_size), 'labels')\n",
    "                self.ph_dropout = tf.placeholder(tf.float32, (), 'dropout')\n",
    "\n",
    "            # Model.\n",
    "            op_logits = self.inference(self.ph_data, self.ph_dropout)\n",
    "            self.op_loss, self.op_loss_average = self.loss(op_logits, self.ph_labels, self.regularization)\n",
    "            self.op_train = self.training(self.op_loss, self.learning_rate,\n",
    "                    self.decay_steps, self.decay_rate, self.momentum)\n",
    "            self.op_prediction = self.prediction(op_logits)\n",
    "\n",
    "            # Initialize variables, i.e. weights and biases.\n",
    "            self.op_init = tf.initialize_all_variables()\n",
    "            \n",
    "            # Summaries for TensorBoard and Save for model parameters.\n",
    "            self.op_summary = tf.merge_all_summaries()\n",
    "            self.op_saver = tf.train.Saver(max_to_keep=5)\n",
    "        \n",
    "        self.graph.finalize()\n",
    "    \n",
    "    def inference(self, data, dropout):\n",
    "        \"\"\"\n",
    "        It builds the model, i.e. the computational graph, as far as\n",
    "        is required for running the network forward to make predictions,\n",
    "        i.e. return logits given raw data.\n",
    "\n",
    "        data: size N x M\n",
    "            N: number of signals (samples)\n",
    "            M: number of vertices (features)\n",
    "        training: we may want to discriminate the two, e.g. for dropout.\n",
    "            True: the model is built for training.\n",
    "            False: the model is built for evaluation.\n",
    "        \"\"\"\n",
    "        # TODO: optimizations for sparse data\n",
    "        logits = self._inference(data, dropout)\n",
    "        return logits\n",
    "    \n",
    "    def probabilities(self, logits):\n",
    "        \"\"\"Return the probability of a sample to belong to each class.\"\"\"\n",
    "        with tf.name_scope('probabilities'):\n",
    "            probabilities = tf.nn.softmax(logits)\n",
    "            return probabilities\n",
    "\n",
    "    def prediction(self, logits):\n",
    "        \"\"\"Return the predicted classes.\"\"\"\n",
    "        with tf.name_scope('prediction'):\n",
    "            prediction = tf.argmax(logits, dimension=1)\n",
    "            return prediction\n",
    "\n",
    "    def loss(self, logits, labels, regularization):\n",
    "        \"\"\"Adds to the inference model the layers required to generate loss.\"\"\"\n",
    "        with tf.name_scope('loss'):\n",
    "            with tf.name_scope('cross_entropy'):\n",
    "                labels = tf.to_int64(labels)\n",
    "                cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)\n",
    "                cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "            with tf.name_scope('regularization'):\n",
    "                regularization *= tf.add_n(self.regularizers)\n",
    "            loss = cross_entropy + regularization\n",
    "            \n",
    "            # Summaries for TensorBoard.\n",
    "            tf.scalar_summary('loss/cross_entropy', cross_entropy)\n",
    "            tf.scalar_summary('loss/regularization', regularization)\n",
    "            tf.scalar_summary('loss/total', loss)\n",
    "            with tf.name_scope('averages'):\n",
    "                averages = tf.train.ExponentialMovingAverage(0.9)\n",
    "                op_averages = averages.apply([cross_entropy, regularization, loss])\n",
    "                tf.scalar_summary('loss/avg/cross_entropy', averages.average(cross_entropy))\n",
    "                tf.scalar_summary('loss/avg/regularization', averages.average(regularization))\n",
    "                tf.scalar_summary('loss/avg/total', averages.average(loss))\n",
    "                with tf.control_dependencies([op_averages]):\n",
    "                    loss_average = tf.identity(averages.average(loss), name='control')\n",
    "            return loss, loss_average\n",
    "    \n",
    "    def training(self, loss, learning_rate, decay_steps, decay_rate=0.95, momentum=0.9):\n",
    "        \"\"\"Adds to the loss model the Ops required to generate and apply gradients.\"\"\"\n",
    "        with tf.name_scope('training'):\n",
    "            # Learning rate.\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            if decay_rate != 1:\n",
    "                learning_rate = tf.train.exponential_decay(\n",
    "                        learning_rate, global_step, decay_steps, decay_rate, staircase=True)\n",
    "            tf.scalar_summary('learning_rate', learning_rate)\n",
    "            # Optimizer.\n",
    "            if momentum == 0:\n",
    "                optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "                #optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "            else:\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "            grads = optimizer.compute_gradients(loss)\n",
    "            op_gradients = optimizer.apply_gradients(grads, global_step=global_step)\n",
    "            # Histograms.\n",
    "            for grad, var in grads:\n",
    "                if grad is None:\n",
    "                    print('warning: {} has no gradient'.format(var.op.name))\n",
    "                else:\n",
    "                    tf.histogram_summary(var.op.name + '/gradients', grad)\n",
    "            # The op return the learning rate.\n",
    "            with tf.control_dependencies([op_gradients]):\n",
    "                op_train = tf.identity(learning_rate, name='control')\n",
    "            return op_train\n",
    "\n",
    "    # Helper methods.\n",
    "\n",
    "    def _weight_variable(self, shape, regularization=True):\n",
    "        initial = tf.truncated_normal_initializer(0, 0.1)\n",
    "        var = tf.get_variable('weights', shape, tf.float32, initializer=initial)\n",
    "        if regularization:\n",
    "            self.regularizers.append(tf.nn.l2_loss(var))\n",
    "        tf.histogram_summary(var.op.name, var)\n",
    "        return var\n",
    "\n",
    "    def _bias_variable(self, shape, regularization=True):\n",
    "        initial = tf.constant_initializer(0.1)\n",
    "        var = tf.get_variable('bias', shape, tf.float32, initializer=initial)\n",
    "        if regularization:\n",
    "            self.regularizers.append(tf.nn.l2_loss(var))\n",
    "        tf.histogram_summary(var.op.name, var)\n",
    "        return var\n",
    "\n",
    "    def _conv2d(self, x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class fc1(base_model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def _inference(self, x, dropout):\n",
    "        W = self._weight_variable([NFEATURES, NCLASSES])\n",
    "        b = self._bias_variable([NCLASSES])\n",
    "        y = tf.matmul(x, W) + b\n",
    "        return y\n",
    "\n",
    "class fc2(base_model):\n",
    "    def __init__(self, nhiddens):\n",
    "        super().__init__()\n",
    "        self.nhiddens = nhiddens\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([NFEATURES, self.nhiddens])\n",
    "            b = self._bias_variable([self.nhiddens])\n",
    "            y = tf.nn.relu(tf.matmul(x, W) + b)\n",
    "        with tf.name_scope('fc2'):\n",
    "            W = self._weight_variable([self.nhiddens, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cnn2(base_model):\n",
    "    \"\"\"Simple convolutional model.\"\"\"\n",
    "    def __init__(self, K, F):\n",
    "        super().__init__()\n",
    "        self.K = K  # Patch size\n",
    "        self.F = F  # Number of features\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('conv1'):\n",
    "            W = self._weight_variable([self.K, self.K, 1, self.F])\n",
    "            b = self._bias_variable([self.F])\n",
    "#            b = self._bias_variable([1, 28, 28, self.F])\n",
    "            x_2d = tf.reshape(x, [-1,28,28,1])\n",
    "            y_2d = self._conv2d(x_2d, W) + b\n",
    "            y_2d = tf.nn.relu(y_2d)\n",
    "        with tf.name_scope('fc1'):\n",
    "            y = tf.reshape(y_2d, [-1, NFEATURES*self.F])\n",
    "            W = self._weight_variable([NFEATURES*self.F, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class fcnn2(base_model):\n",
    "    \"\"\"CNN using the FFT.\"\"\"\n",
    "    def __init__(self, F):\n",
    "        super().__init__()\n",
    "        self.F = F  # Number of features\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('conv1'):\n",
    "            # Transform to Fourier domain\n",
    "            x_2d = tf.reshape(x, [-1, 28, 28])\n",
    "            x_2d = tf.complex(x_2d, 0)\n",
    "            xf_2d = tf.batch_fft2d(x_2d)\n",
    "            xf = tf.reshape(xf_2d, [-1, NFEATURES])\n",
    "            xf = tf.expand_dims(xf, 1)  # NSAMPLES x 1 x NFEATURES\n",
    "            xf = tf.transpose(xf)  # NFEATURES x 1 x NSAMPLES\n",
    "            # Filter\n",
    "            Wreal = self._weight_variable([int(NFEATURES/2), self.F, 1])\n",
    "            Wimg = self._weight_variable([int(NFEATURES/2), self.F, 1])\n",
    "            W = tf.complex(Wreal, Wimg)\n",
    "            xf = xf[:int(NFEATURES/2), :, :]\n",
    "            yf = tf.batch_matmul(W, xf)  # for each feature\n",
    "            yf = tf.concat(0, [yf, tf.conj(yf)])\n",
    "            yf = tf.transpose(yf)  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            yf_2d = tf.reshape(yf, [-1, 28, 28])\n",
    "            # Transform back to spatial domain\n",
    "            y_2d = tf.batch_ifft2d(yf_2d)\n",
    "            y_2d = tf.real(y_2d)\n",
    "            y = tf.reshape(y_2d, [-1, self.F, NFEATURES])\n",
    "            # Bias and non-linearity\n",
    "            b = self._bias_variable([1, self.F, 1])\n",
    "#            b = self._bias_variable([1, self.F, NFEATURES])\n",
    "            y += b  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*NFEATURES, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*NFEATURES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class fgcnn2(base_model):\n",
    "    \"\"\"Graph CNN with full weights, i.e. patch has the same size as input.\"\"\"\n",
    "    def __init__(self, L, F):\n",
    "        super().__init__()\n",
    "        #self.L = L  # Graph Laplacian, NFEATURES x NFEATURES\n",
    "        self.F = F  # Number of filters\n",
    "        _, self.U = graph.fourier(L)\n",
    "    def _inference(self, x, dropout):\n",
    "        # x: NSAMPLES x NFEATURES\n",
    "        with tf.name_scope('gconv1'):\n",
    "            # Transform to Fourier domain\n",
    "            U = tf.constant(self.U, dtype=tf.float32)\n",
    "            xf = tf.matmul(x, U)\n",
    "            xf = tf.expand_dims(xf, 1)  # NSAMPLES x 1 x NFEATURES\n",
    "            xf = tf.transpose(xf)  # NFEATURES x 1 x NSAMPLES\n",
    "            # Filter\n",
    "            W = self._weight_variable([NFEATURES, self.F, 1])\n",
    "            yf = tf.batch_matmul(W, xf)  # for each feature\n",
    "            yf = tf.transpose(yf)  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            yf = tf.reshape(yf, [-1, NFEATURES])\n",
    "            # Transform back to graph domain\n",
    "            Ut = tf.transpose(U)\n",
    "            y = tf.matmul(yf, Ut)\n",
    "            y = tf.reshape(yf, [-1, self.F, NFEATURES])\n",
    "            # Bias and non-linearity\n",
    "            b = self._bias_variable([1, self.F, 1])\n",
    "#            b = self._bias_variable([1, self.F, NFEATURES])\n",
    "            y += b  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*NFEATURES, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*NFEATURES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class lgcnn2_1(base_model):\n",
    "    \"\"\"Graph CNN which uses the Lanczos approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        self.L = L  # Graph Laplacian, M x M\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M, K = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Lanczos basis\n",
    "            xl = tf.reshape(x, [-1, self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xl, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "            b = self._bias_variable([1, 1, self.F])\n",
    "#            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class lgcnn2_2(base_model):\n",
    "    \"\"\"Graph CNN which uses the Lanczos approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        self.L = L  # Graph Laplacian, M x M\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Lanczos basis\n",
    "            xl = tf.transpose(x)  # M x N\n",
    "            def lanczos(x):\n",
    "                return graph.lanczos(self.L, x, self.K)\n",
    "            xl = tf.py_func(lanczos, [xl], [tf.float32])[0]\n",
    "            xl = tf.transpose(xl)  # N x M x K\n",
    "            xl = tf.reshape(xl, [-1, self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xl, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cgcnn2_2(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        self.L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Chebyshev basis\n",
    "            xc = tf.transpose(x)  # M x N\n",
    "            def chebyshev(x):\n",
    "                return graph.chebyshev(self.L, x, self.K)\n",
    "            xc = tf.py_func(chebyshev, [xc], [tf.float32])[0]\n",
    "            xc = tf.transpose(xc)  # N x M x K\n",
    "            xc = tf.reshape(xc, [-1, self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xc, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class cgcnn2_3(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        self.L = L.toarray()\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            def filter(xt, k):\n",
    "                xt = tf.reshape(xt, [-1, 1])  # NM x 1\n",
    "                w = tf.slice(W, [k,0], [1,-1])  # 1 x F\n",
    "                y = tf.matmul(xt, w)  # NM x F\n",
    "                return tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            xt0 = x\n",
    "            y = filter(xt0, 0)\n",
    "            if self.K > 1:\n",
    "                xt1 = tf.matmul(x, self.L, b_is_sparse=True)  # N x M\n",
    "                y += filter(xt1, 1)\n",
    "            for k in range(2, self.K):\n",
    "                xt2 = 2 * tf.matmul(xt1, self.L, b_is_sparse=True) - xt0  # N x M\n",
    "                y += filter(xt2, k)\n",
    "                xt0, xt1 = xt1, xt2\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class cgcnn2_4(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        L = L.tocoo()\n",
    "        data = L.data\n",
    "        indices = np.empty((L.nnz, 2))\n",
    "        indices[:,0] = L.row\n",
    "        indices[:,1] = L.col\n",
    "        L = tf.SparseTensor(indices, data, L.shape)\n",
    "        self.L = tf.sparse_reorder(L)\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            def filter(xt, k):\n",
    "                xt = tf.transpose(xt)  # N x M\n",
    "                xt = tf.reshape(xt, [-1, 1])  # NM x 1\n",
    "                w = tf.slice(W, [k,0], [1,-1])  # 1 x F\n",
    "                y = tf.matmul(xt, w)  # NM x F\n",
    "                return tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            xt0 = tf.transpose(x)  # M x N\n",
    "            y = filter(xt0, 0)\n",
    "            if self.K > 1:\n",
    "                xt1 = tf.sparse_tensor_dense_matmul(self.L, xt0)\n",
    "                y += filter(xt1, 1)\n",
    "            for k in range(2, self.K):\n",
    "                xt2 = 2 * tf.sparse_tensor_dense_matmul(self.L, xt1) - xt0  # M x N\n",
    "                y += filter(xt2, k)\n",
    "                xt0, xt1 = xt1, xt2\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class cgcnn2_5(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        L = L.tocoo()\n",
    "        data = L.data\n",
    "        indices = np.empty((L.nnz, 2))\n",
    "        indices[:,0] = L.row\n",
    "        indices[:,1] = L.col\n",
    "        L = tf.SparseTensor(indices, data, L.shape)\n",
    "        self.L = tf.sparse_reorder(L)\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Chebyshev basis\n",
    "            xt0 = tf.transpose(x)  # M x N\n",
    "            xt = tf.expand_dims(xt0, 0)  # 1 x M x N\n",
    "            def concat(xt, x):\n",
    "                x = tf.expand_dims(x, 0)  # 1 x M x N\n",
    "                return tf.concat(0, [xt, x])  # K x M x N\n",
    "            if self.K > 1:\n",
    "                xt1 = tf.sparse_tensor_dense_matmul(self.L, xt0)\n",
    "                xt = concat(xt, xt1)\n",
    "            for k in range(2, self.K):\n",
    "                xt2 = 2 * tf.sparse_tensor_dense_matmul(self.L, xt1) - xt0  # M x N\n",
    "                xt = concat(xt, xt2)\n",
    "                xt0, xt1 = xt1, xt2\n",
    "            xt = tf.transpose(xt)  # N x M x K\n",
    "            xt = tf.reshape(xt, [-1,self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xt, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bspline_basis(K, x, degree=3):\n",
    "    \"\"\"\n",
    "    Return the B-spline basis.\n",
    "\n",
    "    K: number of control points.\n",
    "    x: evaluation points\n",
    "       or number of evenly distributed evaluation points.\n",
    "    degree: degree of the spline. Cubic spline by default.\n",
    "    \"\"\"\n",
    "    if np.isscalar(x):\n",
    "        x = np.linspace(0, 1, x)\n",
    "\n",
    "    # Evenly distributed knot vectors.\n",
    "    kv1 = x.min() * np.ones(degree)\n",
    "    kv2 = np.linspace(x.min(), x.max(), K-degree+1)\n",
    "    kv3 = x.max() * np.ones(degree)\n",
    "    kv = np.concatenate((kv1, kv2, kv3))\n",
    "\n",
    "    # Cox - DeBoor recursive function to compute one spline over x.\n",
    "    def cox_deboor(k, d):\n",
    "        # Test for end conditions, the rectangular degree zero spline.\n",
    "        if (d == 0):\n",
    "            return ((x - kv[k] >= 0) & (x - kv[k + 1] < 0)).astype(int)\n",
    "\n",
    "        denom1 = kv[k + d] - kv[k]\n",
    "        term1 = 0\n",
    "        if denom1 > 0:\n",
    "            term1 = ((x - kv[k]) / denom1) * cox_deboor(k, d - 1)\n",
    "\n",
    "        denom2 = kv[k + d + 1] - kv[k + 1]\n",
    "        term2 = 0\n",
    "        if denom2 > 0:\n",
    "            term2 = ((-(x - kv[k + d + 1]) / denom2) * cox_deboor(k + 1, d - 1))\n",
    "\n",
    "        return term1 + term2\n",
    "\n",
    "    # Compute basis for each point\n",
    "    basis = np.column_stack([cox_deboor(k, degree) for k in range(K)])\n",
    "    basis[-1,-1] = 1\n",
    "    return basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cgcnn(base_model):\n",
    "    \"\"\"\n",
    "    Graph CNN which uses the Chebyshev approximation.\n",
    "\n",
    "    The following are hyper-parameters of graph convolutional layers.\n",
    "    They are lists, which length is equal to the number of gconv layers.\n",
    "        F: Number of features.\n",
    "        K: List of polynomial orders, i.e. filter sizes or number of hopes.\n",
    "        p: Pooling size.\n",
    "           Should be 1 (no pooling) or a power of 2 (reduction by 2 at each coarser level).\n",
    "           Beware to have coarsened enough.\n",
    "\n",
    "    L: List of Graph Laplacians. Size M x M. One per coarsening level.\n",
    "\n",
    "    The following are hyper-parameters of fully connected layers.\n",
    "    They are lists, which length is equal to the number of fc layers.\n",
    "        M: Number of features per sample, i.e. number of hidden neurons.\n",
    "           The last layer is the softmax, i.e. M[-1] is the number of classes.\n",
    "    \n",
    "    The following are choices of implementation for various blocks.\n",
    "        filter: filtering operation, e.g. chebyshev5, lanczos2 etc.\n",
    "        brelu: bias and relu, e.g. b1relu or b2relu.\n",
    "        pool: pooling, e.g. mpool1.\n",
    "    \n",
    "    Training parameters:\n",
    "        num_epochs:    Number of training epochs.\n",
    "        learning_rate: Initial learning rate.\n",
    "        decay_rate:    Base of exponential decay. No decay with 1.\n",
    "        decay_steps:   Number of steps after which the learning rate decays.\n",
    "        momentum:      Momentum. 0 indicates no momentum.\n",
    "\n",
    "    Regularization parameters:\n",
    "        regularization: L2 regularizations of weights and biases.\n",
    "        dropout:        Dropout (fc layers): probability to keep hidden neurons. No dropout with 1.\n",
    "        batch_size:     Batch size. Must divide evenly into the dataset sizes.\n",
    "        eval_frequency: Number of steps between evaluations.\n",
    "\n",
    "    Directories:\n",
    "        dir_name: Name for directories (summaries and model parameters).\n",
    "    \"\"\"\n",
    "    def __init__(self, L, F, K, p, M, filter='chebyshev5', brelu='b1relu', pool='mpool1',\n",
    "                num_epochs=20, learning_rate=0.1, decay_rate=0.95, decay_steps=None, momentum=0.9,\n",
    "                regularization=0, dropout=0, batch_size=100, eval_frequency=200,\n",
    "                dir_name=''):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Verify the consistency w.r.t. the number of layers.\n",
    "        assert len(L) >= len(F) == len(K) == len(p)\n",
    "        assert np.all(np.array(p) >= 1)\n",
    "        p_log2 = np.where(np.array(p) > 1, np.log2(p), 0)\n",
    "        assert np.all(np.mod(p_log2, 1) == 0)  # Powers of 2.\n",
    "        assert len(L) >= 1 + np.sum(p_log2)  # Enough coarsening levels for pool sizes.\n",
    "        \n",
    "        # Keep the useful Laplacians only. May be zero.\n",
    "        M_0 = L[0].shape[0]\n",
    "        j = 0\n",
    "        self.L = []\n",
    "        for pp in p:\n",
    "            self.L.append(L[j])\n",
    "            j += int(np.log2(pp)) if pp > 1 else 0\n",
    "        L = self.L\n",
    "        \n",
    "        # Print information about NN architecture.\n",
    "        Ngconv = len(p)\n",
    "        Nfc = len(M)\n",
    "        print('NN architecture')\n",
    "        print('  input: M_0 = {}'.format(M_0))\n",
    "        for i in range(Ngconv):\n",
    "            print('  layer {0}: cgconv{0}'.format(i+1))\n",
    "            print('    representation: M_{0} * F_{0} / p_{0} = {1} * {2} / {3} = {4}'.format(\n",
    "                    i+1, L[i].shape[0], F[i], p[i], L[i].shape[0]*F[i]//p[i]))\n",
    "            F_last = F[i-1] if i > 0 else 1\n",
    "            print('    weights: F_{1} * F_{0} * K_{0} = {2} * {3} * {4} = {5}'.format(\n",
    "                    i+1, i, F_last, F[i], K[i], F_last*F[i]*K[i]))\n",
    "            if brelu == 'b1relu':\n",
    "                print('    biases: F_{} = {}'.format(i+1, F[i]))\n",
    "            elif brelu == 'b2relu':\n",
    "                print('    biases: M_{0} * F_{0} = {1} * {2} = {3}'.format(\n",
    "                        i+1, L[i].shape[0], F[i], L[i].shape[0]*F[i]))\n",
    "        for i in range(Nfc):\n",
    "            name = 'logits (softmax)' if i == Nfc-1 else 'fc{}'.format(i+1)\n",
    "            print('  layer {}: {}'.format(Ngconv+i+1, name))\n",
    "            print('    representation: M_{} = {}'.format(Ngconv+i+1, M[i]))\n",
    "            M_last = M[i-1] if i > 0 else M_0 if Ngconv == 0 else L[-1].shape[0] * F[-1] // p[-1]\n",
    "            print('    weights: M_{} * M_{} = {} * {} = {}'.format(\n",
    "                    Ngconv+i, Ngconv+i+1, M_last, M[i], M_last*M[i]))\n",
    "            print('    biases: M_{} = {}'.format(Ngconv+i+1, M[i]))\n",
    "        \n",
    "        # Store attributes and bind operations.\n",
    "        self.L, self.F, self.K, self.p, self.M = L, F, K, p, M\n",
    "        self.num_epochs, self.learning_rate = num_epochs, learning_rate\n",
    "        self.decay_rate, self.decay_steps, self.momentum = decay_rate, decay_steps, momentum\n",
    "        self.regularization, self.dropout = regularization, dropout\n",
    "        self.batch_size, self.eval_frequency = batch_size, eval_frequency\n",
    "        self.dir_name = dir_name\n",
    "        self.filter = getattr(self, filter)\n",
    "        self.brelu = getattr(self, brelu)\n",
    "        self.pool = getattr(self, pool)\n",
    "        \n",
    "        # Build the computational graph.\n",
    "        self.build_graph(M_0)\n",
    "        \n",
    "    def filter_in_fourier(self, x, L, Fout, K, U, W):\n",
    "        # TODO: N x F x M would avoid the permutations\n",
    "        N, M, Fin = x.get_shape()\n",
    "        N, M, Fin = int(N), int(M), int(Fin)\n",
    "        x = tf.transpose(x, perm=[1, 2, 0])  # M x Fin x N\n",
    "        # Transform to Fourier domain\n",
    "        x = tf.reshape(x, [M, Fin*N])  # M x Fin*N\n",
    "        x = tf.matmul(U, x)  # M x Fin*N\n",
    "        x = tf.reshape(x, [M, Fin, N])  # M x Fin x N\n",
    "        # Filter\n",
    "        x = tf.batch_matmul(W, x)  # for each feature\n",
    "        x = tf.transpose(x)  # N x Fout x M\n",
    "        x = tf.reshape(x, [N*Fout, M])  # N*Fout x M\n",
    "        # Transform back to graph domain\n",
    "        x = tf.matmul(x, U)  # N*Fout x M\n",
    "        x = tf.reshape(x, [N, Fout, M])  # N x Fout x M\n",
    "        return tf.transpose(x, perm=[0, 2, 1])  # N x M x Fout\n",
    "\n",
    "    def fourier(self, x, L, Fout, K):\n",
    "        assert K == L.shape[0]  # artificial but useful to compute number of parameters\n",
    "        N, M, Fin = x.get_shape()\n",
    "        N, M, Fin = int(N), int(M), int(Fin)\n",
    "        # Fourier basis\n",
    "        _, U = graph.fourier(L)\n",
    "        U = tf.constant(U.T, dtype=tf.float32)\n",
    "        # Weights\n",
    "        W = self._weight_variable([M, Fout, Fin], regularization=False)\n",
    "        return self.filter_in_fourier(x, L, Fout, K, U, W)\n",
    "\n",
    "    def spline(self, x, L, Fout, K):\n",
    "        N, M, Fin = x.get_shape()\n",
    "        N, M, Fin = int(N), int(M), int(Fin)\n",
    "        # Fourier basis\n",
    "        lamb, U = graph.fourier(L)\n",
    "        U = tf.constant(U.T, dtype=tf.float32)  # M x M\n",
    "        # Spline basis\n",
    "        B = bspline_basis(K, lamb, degree=3)  # M x K\n",
    "        #B = bspline_basis(K, len(lamb), degree=3)  # M x K\n",
    "        B = tf.constant(B, dtype=tf.float32)\n",
    "        # Weights\n",
    "        W = self._weight_variable([K, Fout*Fin], regularization=False)\n",
    "        W = tf.matmul(B, W)  # M x Fout*Fin\n",
    "        W = tf.reshape(W, [M, Fout, Fin])\n",
    "        return self.filter_in_fourier(x, L, Fout, K, U, W)\n",
    "\n",
    "    def chebyshev2(self, x, L, Fout, K):\n",
    "        \"\"\"\n",
    "        Filtering with Chebyshev interpolation\n",
    "        Implementation: numpy.\n",
    "        \n",
    "        Data: x of size N x M x F\n",
    "            N: number of signals\n",
    "            M: number of vertices\n",
    "            F: number of features per signal per vertex\n",
    "        \"\"\"\n",
    "        N, M, Fin = x.get_shape()\n",
    "        N, M, Fin = int(N), int(M), int(Fin)\n",
    "        # Rescale Laplacian. Copy to not modify the shared L.\n",
    "        L = scipy.sparse.csr_matrix(L)\n",
    "        L = graph.rescale_L(L, lmax=2)\n",
    "        # Transform to Chebyshev basis\n",
    "        x = tf.transpose(x, perm=[1, 2, 0])  # M x Fin x N\n",
    "        x = tf.reshape(x, [M, Fin*N])  # M x Fin*N\n",
    "        def chebyshev(x):\n",
    "            return graph.chebyshev(L, x, K)\n",
    "        x = tf.py_func(chebyshev, [x], [tf.float32])[0]  # K x M x Fin*N\n",
    "        x = tf.reshape(x, [K, M, Fin, N])  # K x M x Fin x N\n",
    "        x = tf.transpose(x, perm=[3,1,2,0])  # N x M x Fin x K\n",
    "        x = tf.reshape(x, [N*M, Fin*K])  # N*M x Fin*K\n",
    "        # Filter: Fin*Fout filters of order K, i.e. one filterbank per feature.\n",
    "        W = self._weight_variable([Fin*K, Fout], regularization=False)\n",
    "        x = tf.matmul(x, W)  # N*M x Fout\n",
    "        return tf.reshape(x, [N, M, Fout])  # N x M x Fout\n",
    "\n",
    "    def chebyshev5(self, x, L, Fout, K):\n",
    "        N, M, Fin = x.get_shape()\n",
    "        N, M, Fin = int(N), int(M), int(Fin)\n",
    "        # Rescale Laplacian and store as a TF sparse tensor. Copy to not modify the shared L.\n",
    "        L = scipy.sparse.csr_matrix(L)\n",
    "        L = graph.rescale_L(L, lmax=2)\n",
    "        L = L.tocoo()\n",
    "        indices = np.column_stack((L.row, L.col))\n",
    "        L = tf.SparseTensor(indices, L.data, L.shape)\n",
    "        L = tf.sparse_reorder(L)\n",
    "        # Transform to Chebyshev basis\n",
    "        x0 = tf.transpose(x, perm=[1, 2, 0])  # M x Fin x N\n",
    "        x0 = tf.reshape(x0, [M, Fin*N])  # M x Fin*N\n",
    "        x = tf.expand_dims(x0, 0)  # 1 x M x Fin*N\n",
    "        def concat(x, x_):\n",
    "            x_ = tf.expand_dims(x_, 0)  # 1 x M x Fin*N\n",
    "            return tf.concat(0, [x, x_])  # K x M x Fin*N\n",
    "        if K > 1:\n",
    "            x1 = tf.sparse_tensor_dense_matmul(L, x0)\n",
    "            x = concat(x, x1)\n",
    "        for k in range(2, K):\n",
    "            x2 = 2 * tf.sparse_tensor_dense_matmul(L, x1) - x0  # M x Fin*N\n",
    "            x = concat(x, x2)\n",
    "            x0, x1 = x1, x2\n",
    "        x = tf.reshape(x, [K, M, Fin, N])  # K x M x Fin x N\n",
    "        x = tf.transpose(x, perm=[3,1,2,0])  # N x M x Fin x K\n",
    "        x = tf.reshape(x, [N*M, Fin*K])  # N*M x Fin*K\n",
    "        # Filter: Fin*Fout filters of order K, i.e. one filterbank per feature pair.\n",
    "        W = self._weight_variable([Fin*K, Fout], regularization=False)\n",
    "        x = tf.matmul(x, W)  # N*M x Fout\n",
    "        return tf.reshape(x, [N, M, Fout])  # N x M x Fout\n",
    "\n",
    "    def b1relu(self, x):\n",
    "        \"\"\"Bias and ReLU. One bias per filter.\"\"\"\n",
    "        N, M, F = x.get_shape()\n",
    "        b = self._bias_variable([1, 1, int(F)], regularization=False)\n",
    "        return tf.nn.relu(x + b)\n",
    "\n",
    "    def b2relu(self, x):\n",
    "        \"\"\"Bias and ReLU. One bias per vertex per filter.\"\"\"\n",
    "        N, M, F = x.get_shape()\n",
    "        b = self._bias_variable([1, int(M), int(F)], regularization=False)\n",
    "        return tf.nn.relu(x + b)\n",
    "\n",
    "    def mpool1(self, x, p):\n",
    "        \"\"\"Max pooling of size p. Should be a power of 2.\"\"\"\n",
    "        if p > 1:\n",
    "            x = tf.expand_dims(x, 3)  # N x M x F x 1\n",
    "            x = tf.nn.max_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n",
    "            #tf.maximum\n",
    "            return tf.squeeze(x, [3])  # N x M/p x F\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def fc(self, x, Mout, relu=True):\n",
    "        \"\"\"Fully connected layer with Mout features.\"\"\"\n",
    "        N, Min = x.get_shape()\n",
    "        W = self._weight_variable([int(Min), Mout], regularization=True)\n",
    "        b = self._bias_variable([Mout], regularization=True)\n",
    "        x = tf.matmul(x, W) + b\n",
    "        return tf.nn.relu(x) if relu else x\n",
    "\n",
    "    def _inference(self, x, dropout):\n",
    "        # Graph convolutional layers.\n",
    "        x = tf.expand_dims(x, 2)  # N x M x F=1\n",
    "        for i in range(len(self.p)):\n",
    "            with tf.variable_scope('cgconv{}'.format(i+1)):\n",
    "                with tf.name_scope('filter'):\n",
    "                    x = self.filter(x, self.L[i], self.F[i], self.K[i])\n",
    "                with tf.name_scope('bias_relu'):\n",
    "                    x = self.brelu(x)\n",
    "                with tf.name_scope('pooling'):\n",
    "                    x = self.pool(x, self.p[i])\n",
    "        \n",
    "        # Fully connected hidden layers.\n",
    "        N, M, F = x.get_shape()\n",
    "        x = tf.reshape(x, [int(N), int(M*F)])  # N x M\n",
    "        for i,M in enumerate(self.M[:-1]):\n",
    "            with tf.variable_scope('fc{}'.format(i+1)):\n",
    "                x = self.fc(x, M)\n",
    "                x = tf.nn.dropout(x, dropout)\n",
    "        \n",
    "        # Logits linear layer, i.e. softmax without normalization.\n",
    "        with tf.variable_scope('logits'):\n",
    "            x = self.fc(x, self.M[-1], relu=False)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
